@article{adcock_collier01a,
  title = {Measurement {{Validity}}: {{A Shared Standard}} for {{Qualitative}} and {{Quantitative Research}}},
  shorttitle = {Measurement {{Validity}}},
  author = {Adcock, Robert and Collier, David},
  year = {2001},
  month = sep,
  journal = {American Political Science Review},
  volume = {95},
  number = {3},
  pages = {529--546},
  issn = {0003-0554, 1537-5943},
  doi = {10.1017/S0003055401003100},
  urldate = {2025-05-19},
  abstract = {Scholars routinely make claims that presuppose the validity of the observations and measurements that operationalize their concepts. Yet, despite recent advances in political science methods, surprisingly little attention has been devoted to measurement validity. We address this gap by exploring four themes. First, we seek to establish a shared framework that allows quantitative and qualitative scholars to assess more effectively, and communicate about, issues of valid measurement. Second, we underscore the need to draw a clear distinction between measurement issues and disputes about concepts. Third, we discuss the contextual specificity of measurement claims, exploring a variety of measurement strategies that seek to combine generality and validity by devoting greater attention to context. Fourth, we address the proliferation of terms for alternative measurement validation procedures and offer an account of the three main types of validation most relevant to political scientists.},
  langid = {english},
  file = {/home/ral/Zotero/storage/MND4IYWN/Adcock and Collier - 2001 - Measurement Validity A Shared Standard for Qualitative and Quantitative Research.pdf}
}

@article{ansolabehere_etal08a,
  title = {The {{Strength}} of {{Issues}}: {{Using Multiple Measures}} to {{Gauge Preference Stability}}, {{Ideological Constraint}}, and {{Issue Voting}}},
  shorttitle = {The {{Strength}} of {{Issues}}},
  author = {Ansolabehere, Stephen and Rodden, Jonathan and Snyder, James M.},
  year = {2008},
  month = may,
  journal = {American Political Science Review},
  volume = {102},
  number = {2},
  pages = {215--232},
  issn = {0003-0554, 1537-5943},
  doi = {10.1017/S0003055408080210},
  urldate = {2025-06-05},
  abstract = {A venerable supposition of American survey research is that the vast majority of voters have incoherent and unstable preferences about political issues, which in turn have little impact on vote choice. We demonstrate that these findings are manifestations of measurement error associated with individual survey items. First, we show that averaging a large number of survey items on the same broadly defined issue area---for example, government involvement in the economy, or moral issues---eliminates a large amount of measurement error and reveals issue preferences that are well structured and stable. This stability increases steadily as the number of survey items increases and can approach that of party identification. Second, we show that once measurement error has been reduced through the use of multiple measures, issue preferences have much greater explanatory power in models of presidential vote choice, again approaching that of party identification.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/home/ral/Zotero/storage/UPM3654V/Ansolabehere et al. - 2008 - The Strength of Issues Using Multiple Measures to Gauge Preference Stability, Ideological Constrain.pdf}
}

@article{artstein_poesio08,
  title = {Inter-{{Coder Agreement}} for {{Computational Linguistics}}},
  author = {Artstein, Ron and Poesio, Massimo},
  year = {2008},
  month = dec,
  journal = {Computational Linguistics},
  volume = {34},
  number = {4},
  pages = {555--596},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli.07-034-R2},
  urldate = {2025-05-25},
  abstract = {This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff's alpha as well as Scott's pi and Cohen's kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks---but that their use makes the interpretation of the value of the coefficient even harder.},
  langid = {english},
  file = {/home/ral/Zotero/storage/EYW3E44A/Artstein et Poesio - 2008 - Inter-Coder Agreement for Computational Linguistics.pdf}
}

@article{barrie_etal24,
  title = {Replication for {{Language Models}}: {{Problems}}, {{Principles}}, and {{Best Practices}} for {{Political Science}}},
  author = {Barrie, Christopher and Palmer, Alexis and Spirling, Arthur},
  year = {2024},
  abstract = {Large Language Models (LMs) are exciting tools: they require minimal researcher input and but make it possible to annotate and generate large quantities of data. Yet there has been almost no systematic research into the reproducibility of research using LMs. This is a potential problem for scientific integrity. We give a theoretical framework for replication in the discipline and show that LM work is perhaps uniquely problematic. We demonstrate the problem empirically using a rolling iterated replication design in which we compare crowdsourcing and LMs on multiple repeated tasks, over many months. We find that LMs can be accurate, but the observed variance in performance is often unacceptably high. Strict ``temperature'' control does not resolve these issues. This affects downstream results. In many cases the LM findings cannot be re-run, let alone replicated. We conclude with recommendations for best practice, including the use of locally versioned `open' LMs.},
  langid = {english},
  file = {/home/ral/Zotero/storage/U9EGFDME/Barrie et al. - Replication for Language Models.pdf}
}

@inproceedings{benoit_etal25,
  title = {Replacing Experts with {{LLMs}} When Analyzing Political Texts},
  booktitle = {{{NASP International}} and {{Interdisciplinary Seminars}}},
  author = {Benoit, Kenneth and De Marchi, Scott and Laver, Conor and Laver, Michael and Ma, Jinshuai},
  year = {2025},
  address = {University of Milan}
}

@misc{bhaduri_etal24,
  title = {Reconciling {{Methodological Paradigms}}: {{Employing Large Language Models}} as {{Novice Qualitative Research Assistants}} in {{Talent Management Research}}},
  shorttitle = {Reconciling {{Methodological Paradigms}}},
  author = {Bhaduri, Sreyoshi and Kapoor, Satya and Gil, Alex and Mittal, Anshul and Mulkar, Rutu},
  year = {2024},
  month = aug,
  number = {arXiv:2408.11043},
  eprint = {2408.11043},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.11043},
  urldate = {2025-06-26},
  abstract = {Qualitative data collection and analysis approaches, such as those employing interviews and focus groups, provide rich insights into customer attitudes, sentiment, and behavior. However, manually analyzing qualitative data requires extensive time and effort to identify relevant topics and thematic insights. This study proposes a novel approach to address this challenge by leveraging Retrieval Augmented Generation (RAG) based Large Language Models (LLMs) for analyzing interview transcripts. The novelty of this work lies in strategizing the research inquiry as one that is augmented by an LLM that serves as a novice research assistant. This research explores the mental model of LLMs to serve as novice qualitative research assistants for researchers in the talent management space. A RAG-based LLM approach is extended to enable topic modeling of semi-structured interview data, showcasing the versatility of these models beyond their traditional use in information retrieval and search. Our findings demonstrate that the LLM-augmented RAG approach can successfully extract topics of interest, with significant coverage compared to manually generated topics from the same dataset. This establishes the viability of employing LLMs as novice qualitative research assistants. Additionally, the study recommends that researchers leveraging such models lean heavily on quality criteria used in traditional qualitative research to ensure rigor and trustworthiness of their approach. Finally, the paper presents key recommendations for industry practitioners seeking to reconcile the use of LLMs with established qualitative research paradigms, providing a roadmap for the effective integration of these powerful, albeit novice, AI tools in the analysis of qualitative datasets within talent},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  file = {/home/ral/Zotero/storage/PNM5SI6Y/Bhaduri et al. - 2024 - Reconciling Methodological Paradigms Employing Large Language Models as Novice Qualitative Research.pdf;/home/ral/Zotero/storage/L4NXM2RI/2408.html}
}

@book{bickman_rog08,
  title = {The {{SAGE Handbook}} of {{Applied Social Research Methods}}},
  author = {Bickman, Leonard and Rog, Debra J.},
  year = {2008},
  publisher = {SAGE Publications},
  abstract = {The Second Edition of The SAGE Handbook of Applied Social Research Methods provides students and researchers with the most comprehensive resource covering core methods, research designs, and data collection, management, and analysis issues. This thoroughly revised edition continues to place critical emphasis on finding the tools that best fit the research question given the constraints of deadlines, budget, and available staff. Each chapter offers guidance on how to make intelligent and conscious tradeoffs so that one can refine and hone the research question as new knowledge is gained, unanticipated obstacles are encountered, or contextual shifts take place. Each chapter has been enhanced pedagogically to include more step-by-step procedures, more practical examples from various settings to illustrate the method, parameters to define when the method is most appropriate and when it is not appropriate. The editors also include numerous graphs, models, tip boxes to provide teaching and learning tools.Key Features of the Second EditionEmphasizes applying research techniques, particularly in "real-world" settings in which there are various data, money, time, and political constraintsContains new chapters on mixed methods, qualitative comparative analysis, concept mapping, and internet data collectionOffers a newly developed section that serves as a guide for students who are attempting to translate the content in the chapters into actionIntended AudienceThis Handbook is appropriate for introductory and intermediate research methods courses that focus intently on practical applications and a survey of the many methods available to budding researchers.},
  googlebooks = {m4\_MAwAAQBAJ},
  isbn = {978-1-4129-7331-1},
  langid = {english},
  keywords = {Reference / Research,Social Science / Research}
}

@book{bradburn_etal04a,
  title = {Asking {{Questions}}: {{The Definitive Guide}} to {{Questionnaire Design}} -- {{For Market Research}}, {{Political Polls}}, and {{Social}} and {{Health Questionnaires}}},
  shorttitle = {Asking {{Questions}}},
  author = {Bradburn, Norman M. and Sudman, Seymour and Wansink, Brian},
  year = {2004},
  month = may,
  publisher = {John Wiley \& Sons},
  abstract = {Since it was first published more than twenty-five years ago, Asking Questions has become a classic guide for designing questionnaires3{\textfractionsolidus}4the most widely used method for collecting information about people?s attitudes and behavior. An essential tool for market researchers advertisers, pollsters, and social scientists, this thoroughly updated and definitive work combines time-proven techniques with the most current research, findings, and methods. The book presents a cognitive approach to questionnaire design and includes timely information on the Internet and electronic resources. Comprehensive and concise, Asking Questions can be used to design questionnaires for any subject area, whether administered by telephone, online, mail, in groups, or face-to-face. The book describes the design process from start to finish and is filled with illustrative examples from actual surveys.},
  googlebooks = {YXKbTx2j9i4C},
  isbn = {978-0-7879-7343-8},
  langid = {english},
  keywords = {Business & Economics / Advertising & Promotion,Business & Economics / General,Business & Economics / Marketing / Research}
}

@misc{cheng_etal23,
  title = {Is {{GPT-4}} a {{Good Data Analyst}}?},
  author = {Cheng, Liying and Li, Xingxuan and Bing, Lidong},
  year = {2023},
  month = oct,
  number = {arXiv:2305.15038},
  eprint = {2305.15038},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.15038},
  urldate = {2025-05-25},
  abstract = {As large language models (LLMs) have demonstrated their powerful capabilities in plenty of domains and tasks, including context understanding, code generation, language generation, data storytelling, etc., many data analysts may raise concerns if their jobs will be replaced by artificial intelligence (AI). This controversial topic has drawn great attention in public. However, we are still at a stage of divergent opinions without any definitive conclusion. Motivated by this, we raise the research question of ``is GPT-4 a good data analyst?'' in this work and aim to answer it by conducting head-to-head comparative studies. In detail, we regard GPT-4 as a data analyst to perform end-to-end data analysis with databases from a wide range of domains. We propose a framework to tackle the problems by carefully designing the prompts for GPT-4 to conduct experiments. We also design several task-specific evaluation metrics to systematically compare the performances between several professional human data analysts and GPT-4. Experimental results show that GPT-4 can achieve comparable performance to humans. We also provide in-depth discussions about our results to shed light on further studies before reaching the conclusion that GPT-4 can replace data analysts. Our code, data and demo are available at: https://github.com/DAMO-NLP-SG/ GPT4-as-DataAnalyst.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/ral/Zotero/storage/ID49T6CJ/Cheng et al. - 2023 - Is GPT-4 a Good Data Analyst.pdf}
}

@misc{chew_etal23,
  title = {{{LLM-Assisted Content Analysis}}: {{Using Large Language Models}} to {{Support Deductive Coding}}},
  shorttitle = {{{LLM-Assisted Content Analysis}}},
  author = {Chew, Robert and Bollenbacher, John and Wenger, Michael and Speer, Jessica and Kim, Annice},
  year = {2023},
  month = jun,
  number = {arXiv:2306.14924},
  eprint = {2306.14924},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.14924},
  urldate = {2025-05-25},
  abstract = {Deductive coding is a widely used qualitative research method for determining the prevalence of themes across documents. While useful, deductive coding is often burdensome and time consuming since it requires researchers to read, interpret, and reliably categorize a large body of unstructured text documents. Large language models (LLMs), like ChatGPT, are a class of quickly evolving AI tools that can perform a range of natural language processing and reasoning tasks. In this study, we explore the use of LLMs to reduce the time it takes for deductive coding while retaining the flexibility of a traditional content analysis. We outline the proposed approach, called LLM-assisted content analysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on a publicly available deductive coding data set. Additionally, we conduct an empirical benchmark using LACA on 4 publicly available data sets to assess the broader question of how well GPT-3.5 performs across a range of deductive coding tasks. Overall, we find that GPT-3.5 can often perform deductive coding at levels of agreement comparable to human coders. Additionally, we demonstrate that LACA can help refine prompts for deductive coding, identify codes for which an LLM is randomly guessing, and help assess when to use LLMs vs. human coders for deductive coding. We conclude with several implications for future practice of deductive coding and related research methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Applications},
  file = {/home/ral/Zotero/storage/SJTUIX46/Chew et al. - 2023 - LLM-Assisted Content Analysis Using Large Language Models to Support Deductive Coding.pdf}
}

@article{cruz_manata20,
  title = {Measurement of {{Environmental Concern}}: {{A Review}} and {{Analysis}}},
  shorttitle = {Measurement of {{Environmental Concern}}},
  author = {Cruz, Shannon M. and Manata, Brian},
  year = {2020},
  month = mar,
  journal = {Frontiers in Psychology},
  volume = {11},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2020.00363},
  urldate = {2025-06-13},
  abstract = {Recent growth in the subdiscipline of environmental communication has provided many new opportunities for applied communication research in environmental contexts. For authors studying environmental concern or attitudes, however, survey development can be a daunting task. A large number of scales measuring environmental concern have been developed, and it can be challenging to make informed decisions about which to use. To assist authors in navigating the literature, we present a review of existing scales, followed by two studies in which we examine the structural validity of five scales that are commonly implemented in this corpus. These results have important implications for general issues with measurement in this area, and inform our recommendations for authors about key considerations when selecting and using environmental concern scales.},
  langid = {english},
  file = {/home/ral/Zotero/storage/R6Q3LS9F/Cruz and Manata - 2020 - Measurement of Environmental Concern A Review and Analysis.pdf}
}

@article{dillman16,
  title = {{Internet, Phone, Mail and Mixed-Mode Surveys: The Tailored Design Method}},
  author = {Dillman, Don A and Smyth, Jolene D. and Christian, Leah Melani},
  year = {2014},
  journal = {Indianapolis, Indiana},
  langid = {spanish},
  file = {/home/ral/Zotero/storage/MXJ4KC4E/Dillman - 2016 - Internet, Phone, Mail and Mixed-Mode Surveys The Tailored Design Method.pdf}
}

@article{du2024optimizing,
  title = {Optimizing Temperature for Language Models with Multi-Sample Inference},
  author = {Du, Weihua and Yang, Yiming and Welleck, Sean},
  year = {2024}
}

@article{geer88,
  title = {What {{Do Open-Ended Questions Measure}}?},
  author = {Geer, John G.},
  year = {1988},
  journal = {Public Opinion Quarterly},
  volume = {52},
  number = {3},
  pages = {365},
  publisher = {Oxford University Press (OUP)},
  issn = {0033-362X},
  doi = {10.1086/269113},
  urldate = {2025-05-23},
  abstract = {Open-endedquestions are frequentlyused by survey researchersto measurepublicopinion. Some scholars,however, have doubts abouthow accuratelythese kindsof questionsmeasurethe views of the public.A chief concernis thatthe questions tap, in part, people's ability to articulatea response, not their underlyingattitudes. This paper tests whether this concern is warranted.Using open-endedquestionsfromthe CenterforPolitical Studies, I show thatalmostall people respondto open-ended questions. The few individualswho do not respondappearuninterestedin the specificquestionposed, not unableto answersuch questions in general. These findings should increase our confidence in work of scholarswho have relied on open-endedquestions.},
  langid = {english},
  file = {/home/ral/Zotero/storage/68G6FC6Z/Geer - 1988 - What Do Open-Ended Questions Measure.pdf}
}

@article{geer91,
  title = {Do {{Open-Ended Questions Measure}} "{{Salient}}" {{Issues}}?},
  author = {Geer, John G.},
  year = {1991},
  journal = {The Public Opinion Quarterly},
  volume = {55},
  number = {3},
  eprint = {2749307},
  eprinttype = {jstor},
  pages = {360--370},
  abstract = {Closed-ended questions dominate most interview schedules. Yet the almost exclusive use of this form did not arise because open-ended questions, its major competitor, proved to be weak indicators of public opinion. Instead, responses from open-ended questions proved more difficult and expensive to code and analyze than those from closed-ended questions. Although such practical concerns are important, the real task of survey researchers is to measure public opinion accurately. Using an experimental design, this article tests whether open-ended questions measure the important concerns of respondents-one of the long-claimed advantages of this format. The results, on balance, show that open-ended comments reflect such concerns, suggesting that pollsters may want to include more of these questions in their surveys of public opinion.},
  langid = {english},
  file = {/home/ral/Zotero/storage/YPRJ84K2/Geer - 1991 - Do Open-Ended Questions Measure Salient Issues.pdf}
}

@article{grimmer_stewart13a,
  title = {Text as {{Data}}: {{The Promise}} and {{Pitfalls}} of {{Automatic Content Analysis Methods}} for {{Political Texts}}},
  shorttitle = {Text as {{Data}}},
  author = {Grimmer, Justin and Stewart, Brandon M.},
  year = {2013},
  journal = {Political Analysis},
  volume = {21},
  number = {3},
  pages = {267--297},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mps028},
  urldate = {2025-05-25},
  abstract = {Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods---they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/home/ral/Zotero/storage/QIM3S9ZL/Grimmer et Stewart - 2013 - Text as Data The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts.pdf}
}

@article{hainmueller_hopkins14a,
  title = {Public {{Attitudes Toward Immigration}}},
  author = {Hainmueller, Jens and Hopkins, Daniel J.},
  year = {2014},
  month = may,
  journal = {Annual Review of Political Science},
  volume = {17},
  number = {Volume 17, 2014},
  pages = {225--249},
  publisher = {Annual Reviews},
  issn = {1094-2939, 1545-1577},
  doi = {10.1146/annurev-polisci-102512-194818},
  urldate = {2025-06-13},
  abstract = {Immigrant populations in many developed democracies have grown rapidly, and so too has an extensive literature on natives\&apos; attitudes toward immigration. This research has developed from two theoretical foundations, one grounded in political economy, the other in political psychology. These two literatures have developed largely in isolation from one another, yet the conclusions that emerge from each are strikingly similar. Consistently, immigration attitudes show little evidence of being strongly correlated with personal economic circumstances. Instead, research finds that immigration attitudes are shaped by sociotropic concerns about its cultural impacts---and to a lesser extent its economic impacts---on the nation as a whole. This pattern of results has held up as scholars have increasingly turned to experimental tests, and it holds for the United States, Canada, and Western Europe. Still, more work is needed to strengthen the causal identification of sociotropic concerns and to isolate precisely how, when, and why they matter for attitude formation.},
  langid = {english},
  file = {/home/ral/Zotero/storage/4DTEZQZA/Hainmueller and Hopkins - 2014 - Public Attitudes Toward Immigration.pdf;/home/ral/Zotero/storage/HQZ5W7MQ/annurev-polisci-102512-194818.html}
}

@misc{ibm2024temperature,
  title = {What Is {{LLM}} Temperature?},
  author = {{IBM Research}},
  year = {2024}
}

@article{iyengar96a,
  title = {Framing {{Responsibility}} for {{Political Issues}}},
  author = {Iyengar, Shanto},
  year = {1996},
  journal = {Annals of the American Academy of Political and Social Science},
  volume = {546},
  eprint = {1048170},
  eprinttype = {jstor},
  pages = {59--70},
  abstract = {T:his article examines the influence of television news on viewers' attributions of responsibility for political issues. Television's systematic reliance on episodic as opposed to thematic depictions of political life elicits individualistic attributions of responsibility for national problems such as poverty and terrorism. These attributions emphasize the actions of private rather than governmental actors. By obscuring the connections between political problems and the actions or inactions of political leaders, television news trivializes political discourse and weakens the accountability of elected officials.},
  langid = {english},
  file = {/home/ral/Zotero/storage/RMN7IKRZ/Iyengar - 1996 - Framing Responsibility for Political Issues.pdf}
}

@article{janus10,
  title = {The {{Influence}} of {{Social Desirability Pressures}} on {{Expressed Immigration Attitudes}}},
  author = {Janus, Alexander L.},
  year = {2010},
  journal = {Social Science Quarterly},
  volume = {91},
  number = {4},
  pages = {928--946},
  issn = {1540-6237},
  doi = {10.1111/j.1540-6237.2010.00742.x},
  urldate = {2025-06-13},
  abstract = {Objective. Immigration scholars have found that the highly educated and political liberals are considerably less likely to support restrictionist immigration policies than other groups. I ask whether the influence of social desirability pressures in the survey interview is responsible for this finding. Methods. An unobtrusive questioning technique known as the list experiment is used to measure Americans' support for immigration restrictionism. The list experiment can easily be embedded in a standard telephone survey and has been used by previous investigators to study racial attitudes. Results. Restrictionist sentiments are found to be more widespread among the U.S. populace than previous studies have estimated, especially among college graduates and political liberals. Conclusion. My findings have implications for immigration scholars and social scientists who study other sensitive attitudes and behaviors. The most commonly employed strategies to reduce socially desirable responding may not be enough.},
  copyright = {{\copyright} 2010 by the Southwestern Social Science Association},
  langid = {english},
  file = {/home/ral/Zotero/storage/PNRRMMFV/Janus - 2010 - The Influence of Social Desirability Pressures on Expressed Immigration Attitudes.pdf;/home/ral/Zotero/storage/39GWKUMC/j.1540-6237.2010.00742.html}
}

@article{knoll09,
  title = {``{{And Who Is My Neighbor}}?'' {{Religion}} and {{Immigration Policy Attitudes}}},
  shorttitle = {``{{And Who Is My Neighbor}}?},
  author = {Knoll, Benjamin R.},
  year = {2009},
  journal = {Journal for the Scientific Study of Religion},
  volume = {48},
  number = {2},
  pages = {313--331},
  issn = {1468-5906},
  doi = {10.1111/j.1468-5906.2009.01449.x},
  urldate = {2025-06-13},
  abstract = {This study explores immigration reform as a possible new ``moral'' issue upon which American religious elites and organizations take public positions. It is argued that religion is a key independent variable necessary for understanding the determinants of public attitudes regarding immigration policy. Theoretical expectations are formed from the ethnoreligious, religious restructuralism, and minority marginalization frameworks. Quantitative evidence is presented, that demonstrates that those who attend religious services more frequently are more likely to support liberal immigration reform policies. Members of minority religions, notably Jews and Latter-day Saints, are also more likely to empathize with the plight of undocumented immigrants and support liberal immigration reform measures.},
  copyright = {{\copyright} 2009 The Society for the Scientific Study of Religion},
  langid = {english},
  file = {/home/ral/Zotero/storage/P8NV8N45/Knoll - 2009 - “And Who Is My Neighbor” Religion and Immigration Policy Attitudes.pdf;/home/ral/Zotero/storage/P8LV2GCS/j.1468-5906.2009.01449.html}
}

@article{krosnick99,
  title = {{Survey Research}},
  author = {Krosnick, Jon A.},
  year = {1999},
  journal = {Annual Review of Psychology},
  volume = {50},
  number = {Volume 50, 1999},
  pages = {537--567},
  publisher = {Annual Reviews},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.50.1.537},
  urldate = {2025-05-23},
  abstract = {▪ Abstract For the first time in decades, conventional wisdom about survey methodology is being challenged on many fronts. The insights gained can not only help psychologists do their research better but also provide useful insights into the basics of social interaction and cognition. This chapter reviews some of the many recent advances in the literature, including the following: New findings challenge a long-standing prejudice against studies with low response rates; innovative techniques for pretesting questionnaires offer opportunities for improving measurement validity; surprising effects of the verbal labels put on rating scale points have been identified, suggesting optimal approaches to scale labeling; respondents interpret questions on the basis of the norms of everyday conversation, so violations of those conventions introduce error; some measurement error thought to have been attributable to social desirability response bias now appears to be due to other factors instead, thus encouraging different approaches to fixing such problems; and a new theory of satisficing in questionnaire responding offers parsimonious explanations for a range of response patterns long recognized by psycholo-gists and survey researchers but previously not well understood.},
  langid = {french},
  file = {/home/ral/Zotero/storage/F4VVNJS7/annurev.psych.50.1.html}
}

@article{lazarsfeld44,
  title = {The {{Controversy}} over {{Detailed Interviews}}---{{An Offer}} for {{Negotiation}}},
  author = {Lazarsfeld, Paul F.},
  year = {1944},
  journal = {Public Opinion Quarterly},
  volume = {8},
  number = {1},
  pages = {38--60},
  issn = {0033-362X},
  doi = {10.1086/265666},
  urldate = {2025-05-23},
  abstract = {FROM the beginning, public opinion research has been badgered by the problem of ``depth.'' Does a simple answer to a simple question really reveal a man's opinion? The controversy, if such it may be called, has grown not only between the critics and the pollsters, but among the pollsters themselves. Two philosophies of research have arisen; one wedded to so-called depth interviewing, the other content with more objective methods of research. In this article Dr. Lazarsfeld looks searchingly at the potential schism. His conclusion is that the difference between the two schools, though valid, need not be crucial; there is, in his opinion, a half-way house.Paul F. Lazarsfeld is Director of Columbia University's Office of Radio Research, and a consultant on problems of public opinion research to various private and government organizations. His writings on radio research are well known to readers of the QUARTERLY.},
  file = {/home/ral/Zotero/storage/MFJNZQWI/1914229.html}
}

@article{likert32a,
  title = {A Technique for the Measurement of Attitudes},
  author = {Likert, R.},
  year = {1932},
  journal = {Archives of Psychology},
  volume = {22  140},
  pages = {55--55},
  abstract = {The project conceived in 1929 by Gardner Murphy and the writer aimed first to present a wide array of problems having to do with five major "attitude areas"---international relations, race relations, economic conflict, political conflict, and religion. The kind of questionnaire material falls into four classes: yes-no, multiple choice, propositions to be responded to by degrees of approval, and a series of brief newspaper narratives to be approved or disapproved in various degrees. The monograph aims to describe a technique rather than to give results. The appendix, covering ten pages, shows the method of constructing an attitude scale. A bibliography is also given. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/home/ral/Zotero/storage/SLM4G27N/1933-01885-001.html}
}

@article{liu_etal25,
  title = {Qualitative {{Coding}} with {{GPT-4}}: {{Where}} It {{Works Better}}},
  shorttitle = {Qualitative {{Coding}} with {{GPT-4}}},
  author = {Liu, Xiner and Zambrano, Andres Felipe and Baker, Ryan S. and Barany, Amanda and Ocumpaugh, Jaclyn and Zhang, Jiayi and Pankiewicz, Maciej and Nasiar, Nidhi and Wei, Zhanlan},
  year = {2025},
  month = mar,
  journal = {Journal of Learning Analytics},
  volume = {12},
  number = {1},
  pages = {169--185},
  issn = {1929-7750},
  doi = {10.18608/jla.2025.8575},
  urldate = {2025-05-25},
  abstract = {This study explores the potential of the large language model GPT-4 as an automated tool for qualitative data analysis by educational researchers, exploring which techniques are most successful for different types of constructs. Specifically, we assess three different prompt engineering strategies --- Zero-shot, Few-shot, and Fewshot with contextual information --- as well as the use of embeddings. We do so in the context of qualitatively coding three distinct educational datasets: Algebra I semi-personalized tutoring session transcripts, student observations in a game-based learning environment, and debugging behaviours in an introductory programming course. We evaluated the performance of each approach based on its inter-rater agreement with human coders and explored how different methods vary in effectiveness depending on a construct's degree of clarity, concreteness, objectivity, granularity, and specificity. Our findings suggest that while GPT-4 can code a broad range of constructs, no single method consistently outperforms the others, and the selection of a particular method should be tailored to the specific properties of the construct and context being analyzed. We also found that GPT-4 has the most difficulty with the same constructs than human coders find more difficult to reach inter-rater reliability on.},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  langid = {english},
  file = {/home/ral/Zotero/storage/8L8HAACA/Liu et al. - 2025 - Qualitative Coding with GPT-4 Where it Works Better.pdf}
}

@article{lombard_etal02,
  title = {Content {{Analysis}} in {{Mass Communication}}: {{Assessment}} and {{Reporting}} of {{Intercoder Reliability}}},
  shorttitle = {Content {{Analysis}} in {{Mass Communication}}},
  author = {Lombard, Matthew and {Snyder-Duch}, Jennifer and Bracken, Cheryl Campanella},
  year = {2002},
  month = oct,
  journal = {Human Communication Research},
  volume = {28},
  number = {4},
  pages = {587--604},
  issn = {0360-3989, 1468-2958},
  doi = {10.1111/j.1468-2958.2002.tb00826.x},
  urldate = {2025-05-25},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  langid = {english},
  file = {/home/ral/Zotero/storage/EEVRKD6W/Lombard et al. - 2002 - Content Analysis in Mass Communication Assessment and Reporting of Intercoder Reliability.pdf}
}

@article{meade_craig12a,
  title = {Identifying Careless Responses in Survey Data.},
  author = {Meade, Adam W. and Craig, S. Bartholomew},
  year = {2012},
  month = sep,
  journal = {Psychological Methods},
  volume = {17},
  number = {3},
  pages = {437--455},
  publisher = {American Psychological Association (APA)},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0028085},
  urldate = {2025-05-23},
  abstract = {When data are collected via anonymous Internet surveys, particularly under conditions of obligatory participation (such as with student samples), data quality can be a concern. However, little guidance exists in the published literature regarding techniques for detecting careless responses. Previously several potential approaches have been suggested for identifying careless respondents via indices computed from the data, yet almost no prior work has examined the relationships among these indicators or the types of data patterns identified by each. In 2 studies, we examined several methods for identifying careless responses, including (a) special items designed to detect careless response, (b) response consistency indices formed from responses to typical survey items, (c) multivariate outlier analysis, (d) response time, and (e) self-reported diligence. Results indicated that there are two distinct patterns of careless response (random and nonrandom) and that different indices are needed to identify these different response patterns. We also found that approximately 10\%--12\% of undergraduates completing a lengthy survey for course credit were identified as careless responders. In Study 2, we simulated data with known random response patterns to determine the efficacy of several indicators of careless response. We found that the nature of the data strongly influenced the efficacy of the indices to identify careless responses. Recommendations include using identified rather than anonymous responses, incorporating instructed response items before data collection, as well as computing consistency indices and multivariate outlier analysis to ensure high-quality data.},
  langid = {english},
  file = {/home/ral/Zotero/storage/XR48HXVX/Meade et Craig - 2012 - Identifying careless responses in survey data..pdf}
}

@article{mens_gallego25,
  title = {Positioning {{Political Texts}} with {{Large Language Models}} by {{Asking}} and {{Averaging}}},
  author = {Mens, Ga{\"e}l Le and Gallego, Aina},
  year = {2025},
  month = jul,
  journal = {Political Analysis},
  volume = {33},
  number = {3},
  pages = {274--282},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2024.29},
  urldate = {2025-06-29},
  abstract = {We use instruction-tuned large language models (LLMs) like GPT-4, Llama 3, MiXtral, or Aya to position political texts within policy and ideological spaces. We ask an LLM where a tweet or a sentence of a political text stands on the focal dimension and take the average of the LLM responses to position political actors such as US Senators, or longer texts such as UK party manifestos or EU policy speeches given in 10 different languages. The correlations between the position estimates obtained with the best LLMs and benchmarks based on text coding by experts, crowdworkers, or roll call votes exceed .90. This approach is generally more accurate than the positions obtained with supervised classifiers trained on large amounts of research data. Using instruction-tuned LLMs to position texts in policy and ideological spaces is fast, cost-efficient, reliable, and reproducible (in the case of open LLMs) even if the texts are short and written in different languages. We conclude with cautionary notes about the need for empirical validation.},
  langid = {english},
  keywords = {ideology,LLM,scaling,text as data},
  file = {/home/ral/Zotero/storage/26S4MKR8/Mens and Gallego - 2025 - Positioning Political Texts with Large Language Models by Asking and Averaging.pdf}
}

@article{repass29,
  title = {Issue {{Salience}} and {{Party Choice}}},
  author = {RePass, David E.},
  year = {1929},
  journal = {American Political Science Review},
  volume = {65},
  number = {2},
  pages = {389--400},
  issn = {0003-0554, 1537-5943},
  doi = {10.2307/1954456},
  urldate = {2025-05-23},
  abstract = {A number of leading studies of voting behavior in recent years have concluded that specific issues are not a salient element in the electoral decision. These studies have indicated not only that voters are unfamiliar with most issues, but also that the electorate is generally unable to detect differences between Republican and Democratic positions on issues. Using the same Survey Research Center interviews upon which these previous findings were based, this article modifies these previous evaluations. This study concentrates on data from the 1964 election ---a campaign that was notable not for the issues it raised, but rather for the public's strong reactions to the candidates. The findings in this article show that, even in 1964, most people were concerned with a number of specific issues and that these issue concerns had a very measurable effect on voting choice. Furthermore, large proportions of people were able accurately to perceive the differences between the parties on those issues that were salient to them. The major reason these findings are so different from previous results is that new measures and a different approach were used---particularly open-ended interview material that for the first time allowed the researcher to discover the issues that were salient to the voter.},
  langid = {english}
}

@article{reveilhac_andmorselli22,
  title = {Dictionary-Based and Machine Learning Classification Approaches: A Comparison for Tonality and Frame Detection on {{Twitter}} Data},
  shorttitle = {Dictionary-Based and Machine Learning Classification Approaches},
  author = {Reveilhac, Maud and {and Morselli}, Davide},
  year = {2022},
  month = dec,
  journal = {Political Research Exchange},
  volume = {4},
  number = {1},
  pages = {2029217},
  publisher = {Routledge},
  issn = {null},
  doi = {10.1080/2474736X.2022.2029217},
  urldate = {2025-05-26},
  abstract = {Automated text analysis methods have made it possible to classify large corpora of text by measures such as frames and tonality, with a growing popularity in social, political and psychological science. These methods often demand a training dataset of sufficient size to generate accurate models that can be applied to unseen texts. In practice, however, there are no clear recommendations about how big the training samples should be. This issue becomes especially acute when dealing with texts skewed toward categories and when researchers cannot afford large samples of annotated texts. Leveraging on the case of support for democracy, we provide a guide to help researchers navigate decisions when producing measures of tonality and frames from a small sample of annotated social media posts. We find that supervised machine learning algorithms outperform dictionaries for tonality classification tasks. However, custom dictionaries are useful complements of these algorithms when identifying latent democracy dimensions in social media messages, especially as the method of elaborating these dictionaries is guided by word embedding techniques and human validation. Therefore, we provide easily implementable recommendations to increase estimation accuracy under non-optimal condition.},
  file = {/home/ral/Zotero/storage/JJBH8QTW/Reveilhac et and Morselli - 2022 - Dictionary-based and machine learning classification approaches a comparison for tonality and frame.pdf}
}

@article{roberts_etal,
  title = {The {{Structural Topic Model}} and {{Applied Social Science}}},
  author = {Roberts, Molly and Stewart, Brandon and Tingley, Dustin and Airoldi, Edoardo},
  year = {2013},
  volume = {4},
  number = {1},
  pages = {1--20},
  langid = {english},
  file = {/home/ral/Zotero/storage/HSDGW78B/Roberts et al. - The Structural Topic Model and Applied Social Science.pdf}
}

@article{roberts_etal14a,
  title = {Structural {{Topic Models}} for {{Open}}-{{Ended Survey Responses}}},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
  year = {2014},
  month = oct,
  journal = {American Journal of Political Science},
  volume = {58},
  number = {4},
  pages = {1064--1082},
  publisher = {Wiley},
  issn = {0092-5853, 1540-5907},
  doi = {10.1111/ajps.12103},
  urldate = {2025-05-23},
  abstract = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/home/ral/Zotero/storage/A56AUND9/Roberts et al. - 2014 - Structural Topic Models for Open‐Ended Survey Responses.pdf}
}

@article{roberts_etal16,
  title = {A {{Model}} of {{Text}} for {{Experimentation}} in the {{Social Sciences}}},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Airoldi, Edoardo M.},
  year = {2016},
  month = jul,
  journal = {Journal of the American Statistical Association},
  volume = {111},
  number = {515},
  pages = {988--1003},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2016.1141684},
  urldate = {2025-05-25},
  abstract = {Statistical models of text have become increasingly popular in statistics and computer science as a method of exploring large document collections. Social scientists often want to move beyond exploration, to measurement and experimentation, and make inference about social and political processes that drive discourse and content. In this article, we develop a model of text data that supports this type of substantive research. Our approach is to posit a hierarchical mixed membership model for analyzing topical content of documents, in which mixing weights are parameterized by observed covariates. In this model, topical prevalence and topical content are specified as a simple generalized linear model on an arbitrary number of documentlevel covariates, such as news source and time of release, enabling researchers to introduce elements of the experimental design that informed document collection into the model, within a generally applicable framework. We demonstrate the proposed methodology by analyzing a collection of news reports about China, where we allow the prevalence of topics to evolve over time and vary across newswire services. Our methods quantify the effect of news wire source on both the frequency and nature of topic coverage. Supplementary materials for this article are available online.},
  langid = {english},
  file = {/home/ral/Zotero/storage/AI34LRHK/Roberts et al. - 2016 - A Model of Text for Experimentation in the Social Sciences.pdf}
}

@misc{salimian_etal25,
  title = {Perceived {{Confidence Scoring}} for {{Data Annotation}} with {{Zero-Shot LLMs}}},
  author = {Salimian, Sina and Uddin, Gias and Jahan, Most Husne and Raza, Shaina},
  year = {2025},
  month = feb,
  number = {arXiv:2502.07186},
  eprint = {2502.07186},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.07186},
  urldate = {2025-06-26},
  abstract = {Zero-shot LLMs are now also used for textual classification tasks, e.g., sentiment/emotion detection of a given input as a sentence/article. However, their performance can be suboptimal in such data annotation tasks. We introduce a novel technique Perceived Confidence Scoring (PCS) that evaluates LLM's confidence for its classification of an input by leveraging Metamorphic Relations (MRs). The MRs generate semantically equivalent yet textually mutated versions of the input. Following the principles of Metamorphic Testing (MT), the mutated versions are expected to have annotation labels similar to the input. By analyzing the consistency of LLM responses across these variations, PCS computes a confidence score based on the frequency of predicted labels. PCS can be used both for single LLM and multiple LLM settings (e.g., majority voting). We introduce an algorithm Perceived Differential Evolution (PDE) that determines the optimal weights assigned to the MRs and the LLMs for a classification task. Empirical evaluation shows PCS significantly improves zero-shot accuracy for Llama-3-8B-Instruct (4.96\%) and Mistral-7B-Instruct-v0.3 (10.52\%), with Gemma-2-9b-it showing a 9.39\% gain. When combining all three models, PCS significantly outperforms majority voting by 7.75\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/ral/Zotero/storage/ZH9GJ87U/Salimian et al. - 2025 - Perceived Confidence Scoring for Data Annotation with Zero-Shot LLMs.pdf;/home/ral/Zotero/storage/HSDUXZEW/2502.html}
}

@misc{sarkar_etal25,
  title = {Conversational {{User-AI Intervention}}: {{A Study}} on {{Prompt Rewriting}} for {{Improved LLM Response Generation}}},
  shorttitle = {Conversational {{User-AI Intervention}}},
  author = {Sarkar, Rupak and Sarrafzadeh, Bahareh and Chandrasekaran, Nirupama and Rangan, Nagu and Resnik, Philip and Yang, Longqi and Jauhar, Sujay Kumar},
  year = {2025},
  month = jun,
  number = {arXiv:2503.16789},
  eprint = {2503.16789},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.16789},
  urldate = {2025-06-29},
  abstract = {Human-LLM conversations are increasingly becoming more pervasive in peoples' professional and personal lives, yet many users still struggle to elicit helpful responses from LLM Chatbots. One of the reasons for this issue is users' lack of understanding in crafting effective prompts that accurately convey their information needs. Meanwhile, the existence of real-world conversational datasets on the one hand, and the text understanding faculties of LLMs on the other, present a unique opportunity to study this problem, and its potential solutions at scale. Thus, in this paper we present the first LLM-centric study of real human-AI chatbot conversations, focused on investigating aspects in which user queries fall short of expressing information needs, and the potential of using LLMs to rewrite suboptimal user prompts. Our findings demonstrate that rephrasing ineffective prompts can elicit better responses from a conversational system, while preserving the user's original intent. Notably, the performance of rewrites improves in longer conversations, where contextual inferences about user needs can be made more accurately. Additionally, we observe that LLMs often need to -- and inherently do -- make {\textbackslash}emph\{plausible\} assumptions about a user's intentions and goals when interpreting prompts. Our findings largely hold true across conversational domains, user intents, and LLMs of varying sizes and families, indicating the promise of using prompt rewriting as a solution for better human-AI interactions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/ral/Zotero/storage/JZS2AZDZ/Sarkar et al. - 2025 - Conversational User-AI Intervention A Study on Prompt Rewriting for Improved LLM Response Generatio.pdf;/home/ral/Zotero/storage/N6AEFZNR/2503.html}
}

@book{schuman_presser96a,
  title = {Questions and {{Answers}} in {{Attitude Surveys}}: {{Experiments}} on {{Question Form}}, {{Wording}}, and {{Context}}},
  shorttitle = {Questions and {{Answers}} in {{Attitude Surveys}}},
  author = {Schuman, Howard and Presser, Stanley},
  year = {1996},
  series = {Sage},
  publisher = {SAGE},
  abstract = {Comprehensive in its coverage, Questions and Answers in Attitude Surveys covers such issues as question order and response order effects; the lack of overlap between respondent-generated categories for open-ended questions and the closed categories generated by research, even with extensive pre-testing with open questions; the effects of explicitly offering respondents a "don't know" or a middle opinion alternative; attitude strength and its relation to reliability; and issues of wording tone.},
  googlebooks = {Je640UKqNaYC},
  isbn = {978-0-7619-0359-8},
  langid = {english},
  keywords = {Reference / Research,Social Science / Research}
}

@article{schuman66,
  title = {The {{Random Probe}}: {{A Technique}} for {{Evaluating}} the {{Validity}} of {{Closed Questions}}},
  shorttitle = {The {{Random Probe}}},
  author = {Schuman, Howard},
  year = {1966},
  journal = {American Sociological Review},
  volume = {31},
  number = {2},
  eprint = {2090907},
  eprinttype = {jstor},
  pages = {218--222},
  publisher = {[American Sociological Association, Sage Publications, Inc.]},
  issn = {0003-1224},
  doi = {10.2307/2090907},
  urldate = {2025-05-23},
  abstract = {The familiar dilemma of open versus closed interview questions becomes especially acute when surveys are undertaken outside middle-class American society. Inevitable ignorance of the subtleties of another culture leads the researcher toward an open-ended approach, while his experience with the difficulties of channeling diverse free responses into a useful frame of reference and of coding enormous masses of verbal data encourages him to rely on closed questions. The method of "random probes" suggested here is intended to allow a survey researcher to eat his cake and still have a little left over.}
}

@article{survey,
  title = {A Survey of Large Language Models},
  author = {Zhao, Wayne and Kun Zhou, Junyi L and Tianyi Tang, Xiaolei Wang and Yupeng Hou, Yingqian Min},
  year = {2023},
  volume = {2},
  number = {1}
}

@article{tai_etal24,
  title = {An {{Examination}} of the {{Use}} of {{Large Language Models}} to {{Aid Analysis}} of {{Textual Data}}},
  author = {Tai, Robert H. and Bentley, Lillian R. and Xia, Xin and Sitt, Jason M. and Fankhauser, Sarah C. and {Chicas-Mosier}, Ana M. and Monteith, Barnas G.},
  year = {2024},
  month = jan,
  journal = {International Journal of Qualitative Methods},
  volume = {23},
  pages = {16094069241231168},
  issn = {1609-4069, 1609-4069},
  doi = {10.1177/16094069241231168},
  urldate = {2025-05-25},
  abstract = {The increasing use of machine learning and Large Language Models (LLMs) opens up opportunities to use these artificially intelligent algorithms in novel ways. This article proposes a methodology using LLMs to support traditional deductive coding in qualitative research. We began our analysis with three different sample texts taken from existing interviews. Next, we created a codebook and inputted the sample text and codebook into an LLM. We asked the LLM to determine if the codes were present in a sample text provided and requested evidence to support the coding. The sample texts were inputted 160 times to record changes between iterations of the LLM response. Each iteration was analogous to a new coder deductively analyzing the text with the codebook information. In our results, we present the outputs for these recursive analyses, along with a comparison of the LLM coding to evaluations made by human coders using traditional coding methods. We argue that LLM analysis can aid qualitative researchers by deductively coding transcripts, providing a systematic and reliable platform for code identification, and offering a means of avoiding analysis misalignment. Implications of using LLM in research praxis are discussed, along with current limitations.},
  langid = {english},
  file = {/home/ral/Zotero/storage/344BKQZT/Tai et al. - 2024 - An Examination of the Use of Large Language Models to Aid Analysis of Textual Data.pdf}
}

@misc{tornberg23a,
  title = {{{ChatGPT-4 Outperforms Experts}} and {{Crowd Workers}} in {{Annotating Political Twitter Messages}} with {{Zero-Shot Learning}}},
  author = {T{\"o}rnberg, Petter},
  year = {2023},
  month = apr,
  number = {arXiv:2304.06588},
  eprint = {2304.06588},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.06588},
  urldate = {2025-05-25},
  abstract = {This paper assesses the accuracy, reliability and bias of the Large Language Model (LLM) ChatGPT-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The LLM is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that ChatGPT-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that LLM will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Social and Information Networks},
  file = {/home/ral/Zotero/storage/XWPPQ465/Törnberg - 2023 - ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-S.pdf}
}

@misc{wei_etal22,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year = {2022},
  month = oct,
  number = {arXiv:2206.07682},
  eprint = {2206.07682},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.07682},
  urldate = {2025-05-25},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/ral/Zotero/storage/UEL82IP2/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf}
}

@misc{wu25,
  title = {Large {{Language Models Can Be}} a {{Viable Substitute}} for {{Expert Political Surveys When}} a {{Shock Disrupts Traditional Measurement Approaches}}},
  author = {Wu, Patrick Y.},
  year = {2025},
  month = jun,
  number = {arXiv:2506.06540},
  eprint = {2506.06540},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.06540},
  urldate = {2025-06-29},
  abstract = {After a disruptive event or shock, such as the Department of Government Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by knowledge of the outcome. This can make it difficult or impossible to reconstruct the pre-event perceptions needed to study the factors associated with the event. This position paper argues that large language models (LLMs), trained on vast amounts of digital media data, can be a viable substitute for expert political surveys when a shock disrupts traditional measurement. We analyze the DOGE layoffs as a specific case study for this position. We use pairwise comparison prompts with LLMs and derive ideology scores for federal executive agencies. These scores replicate pre-layoff expert measures and predict which agencies were targeted by DOGE. We also use this same approach and find that the perceptions of certain federal agencies as knowledge institutions predict which agencies were targeted by DOGE, even when controlling for ideology. This case study demonstrates that using LLMs allows us to rapidly and easily test the associated factors hypothesized behind the shock. More broadly, our case study of this recent event exemplifies how LLMs offer insights into the correlational factors of the shock when traditional measurement techniques fail. We conclude by proposing a two-part criterion for when researchers can turn to LLMs as a substitute for expert political surveys.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/home/ral/Zotero/storage/Z4EB7ATR/Wu - 2025 - Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts.pdf;/home/ral/Zotero/storage/T7UCUDXT/2506.html}
}

@inproceedings{xiao_etal23,
  title = {Supporting {{Qualitative Analysis}} with {{Large Language Models}}: {{Combining Codebook}} with {{GPT-3}} for {{Deductive Coding}}},
  shorttitle = {Supporting {{Qualitative Analysis}} with {{Large Language Models}}},
  booktitle = {28th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Xiao, Ziang and Yuan, Xingdi and Liao, Q. Vera and Abdelghani, Rania and Oudeyer, Pierre-Yves},
  year = {2023},
  eprint = {2304.10548},
  primaryclass = {cs},
  pages = {75--78},
  doi = {10.1145/3581754.3584136},
  urldate = {2025-05-25},
  abstract = {Qualitative analysis of textual contents unpacks rich and valuable information by assigning labels to the data. However, this process is often labor-intensive, particularly when working with large datasets. While recent AI-based tools demonstrate utility, researchers may not have readily available AI resources and expertise, let alone be challenged by the limited generalizability of those task-specific models. In this study, we explored the use of large language models (LLMs) in supporting deductive coding, a major category of qualitative analysis where researchers use pre-determined codebooks to label the data into a fixed set of codes. Instead of training task-specific models, a pre-trained LLM could be used directly for various tasks without fine-tuning through prompt learning. Using a curiosity-driven questions coding task as a case study, we found, by combining GPT-3 with expert-drafted codebooks, our proposed approach achieved fair to substantial agreements with expert-coded results. We lay out challenges and opportunities in using LLMs to support qualitative coding and beyond.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/home/ral/Zotero/storage/U7KMR44T/Xiao et al. - 2023 - Supporting Qualitative Analysis with Large Language Models Combining Codebook with GPT-3 for Deduct.pdf}
}

@article{ziems_etal24a,
  title = {Can {{Large Language Models Transform Computational Social Science}}?},
  author = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
  year = {2024},
  month = mar,
  journal = {Computational Linguistics},
  volume = {50},
  number = {1},
  pages = {237--291},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00502},
  urldate = {2025-05-25},
  abstract = {Abstract             Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that the performance of today's LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.},
  langid = {english},
  file = {/home/ral/Zotero/storage/TP6M7IPX/Ziems et al. - 2024 - Can Large Language Models Transform Computational Social Science.pdf}
}
